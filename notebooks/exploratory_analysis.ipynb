{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\Downloads\\Projects\\mlops\\mem-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "import chromadb\n",
    "import urllib3\n",
    "import accelerate\n",
    "import sentence_transformers\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=True\n",
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=True\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text data from PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.document_loaders.pdf.PyPDFLoader"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an object to load PDF file\n",
    "loader = PyPDFLoader('../data/article.pdf')\n",
    "\n",
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/article.pdf', 'page': 0, 'page_label': '1'}, page_content='The Most Important Skill in the Age of Artificial Intelligence \\nThe COVID-19 pandemic accelerated the pace of digital development worldwide, as \\neverything—from meetings to medical consultations—moved online. This might sound \\noverwhelmingly positive. \\nFor tens of millions of workers, it was not. \\nThey may not have the necessary skills to compete in this new world. These are accountants, \\ntypists, and executive secretaries searching for jobs in a new economy where hired candidates \\nhave titles like \"Cloud Engineer\" or \"Growth Hacker\" on their résumés. Without a concerted \\neffort to retrain them, researchers at RAND Europe have found that they are likely to be left \\nbehind. \\nAnd not just them. The cost of this growing skills gap will be measured in trillions of dollars \\nand will hit hardest in places that lack reliable digital infrastructure, such as internet access or \\nwidespread digital literacy. As the global economy struggles to recover from the impact of \\nCOVID-19, this skills gap threatens to keep pushing it down. \\n“There simply aren’t enough people with the right digital skills to enable the transformation \\nthat companies are seeking,” said Salil Gunashekar, research leader and associate director at \\nRAND Europe, who focuses on science and technology policy. \\nAt some point in the coming years, the world will reach a major milestone: the number of \\nhours worked by machines will equal the number of hours worked by humans. A recent \\nSalesforce survey found that three-quarters of workers worldwide feel unprepared for the jobs \\nthey might encounter on the other side of this milestone. \\nThose planning to work in healthcare or financial services, for example, may need to learn \\nhow to use artificial intelligence-powered computers. Those looking to work in metal mining \\nmay need to understand how to operate robots and analyze Big Data. An accountant may \\nbecome the operator of a process automation robot. \\nBusiness leaders have been warning for years that what they see on résumés does not match \\nwhat they need in new hires. Europe’s Digital Economy and Society Index recently found \\nthat nearly 60% of employers are struggling to fill digital roles with qualified candidates. Yet, \\nthe realities of the pandemic have left them with no choice: four out of five global business \\nleaders say they are accelerating the automation of processes and daily tasks within their \\ncompanies. \\nThe world\\'s leading economies could now lose $11.5 trillion in potential growth by 2028 if \\nthey fail to close the skills gap, according to the global consulting and professional services \\nfirm Accenture. India, South Africa, and Mexico will be particularly affected. So will groups \\nthat can least afford the economic loss: the elderly, racial and ethnic minorities, and people \\nliving in rural areas. \\nThe World Economic Forum estimates that 85 million jobs could be lost to automation in the \\nnext three years across more than a dozen industries. At the same time, it expects 97 million \\nnew jobs to emerge, better suited to the future of work. On paper, this should be a win. But '),\n",
       " Document(metadata={'source': '../data/article.pdf', 'page': 1, 'page_label': '2'}, page_content='without a major commitment to upskilling and retraining existing workers, RAND Europe \\nfound that it will be a loss for employees and a loss for employers. \\nThere are no simple solutions here. Companies need to become more agile in reallocating and \\nredeploying their existing workforce to better meet their needs instead of trying to hire their \\nway out of the skills gap. They also need to do more to help these employees acquire \\ntechnical skills, such as programming and data analysis, as well as interpersonal skills, such \\nas teamwork, that are essential for success. National governments can help by investing in \\nvocational programs and other support for displaced workers. \\nAn important step would be to develop a common \"skills language,\" researchers wrote. This \\nwould ensure that candidates and employers have the same understanding when using terms \\nlike \"Cloud Engineer\" or \"AI Engineer.\" It would help hiring managers quickly assess \\ncandidates based on the skills they bring to the job rather than just the name of the university \\non their résumé (which, in today’s world, is increasingly irrelevant). \\nWorkers, on the other hand, need to change their mindset. Education no longer ends with a \\nhigh school diploma or a college degree. The skills they have now may not be relevant in a \\nfew years. As a technology manager in Canada advised during the research: “Be Good at \\nLearning.” \\nYes. This is the most important skill in the age of Artificial Intelligence: \\n“Be Good at Learning.” \\nDigital transformation requires you to learn, unlearn, relearn, and stay in this cycle if you \\ntruly want to remain employable. The ability to adapt to new technologies and the skill to \\nlearn faster and faster will be what sets you apart from machines. \\nIt doesn’t matter your field, your industry, your degree, your age, or your gender. The world \\nis undergoing a deep digital transformation, and jobs as we know them are being reinvented. \\nThose who fail to keep up with this natural evolution will be left behind, as we have seen \\nmany times throughout human history. Learn as much as you can about different subjects, \\nfrom interpersonal skills to technical skills. The only limit to what you can learn is the one \\nyou impose on yourself. \\n“Be Good at Learning.” Stay in a constant state of learning. \\n ')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the file PDF\n",
    "pages = loader.load()\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content:  without a major commitment to upskilling and retraining existing workers, RAND Europe \n",
      "found that it will be a loss for employees and a loss for employers. \n",
      "There are no simple solutions here. Companies need to become more agile in reallocating and \n",
      "redeploying their existing workforce to better meet their needs instead of trying to hire their \n",
      "way out of the skills gap. They also need to do more to help these employees acquire \n",
      "technical skills, such as programming and data analysis, as well as\n"
     ]
    }
   ],
   "source": [
    "page = pages[1]\n",
    "\n",
    "print(\"Page content: \", page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': '../data/article.pdf', 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Metadata:\", page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text Data in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chunk_size = 1000**: Specifies that each resulting chunk of text will have a maximum of 1000 characters.\n",
    "\n",
    "**chunk_overlap = 20**: Indicates that each chunk will have 20 characters of overlap with the next chunk. This means that the last 20 characters of a chunk will be repeated at the beginning of the next chunk.\n",
    "\n",
    "What it is for:\n",
    "\n",
    "This approach is useful in several situations where large texts need to be processed or analyzed, such as:\n",
    "\n",
    "- Input for language models: Many LLMs have a limit of tokens that they can process in a single iteration. Dividing the text into smaller chunks ensures that the text is sent within the allowed limit.\n",
    "\n",
    "- Data analysis and indexing: It is common in search engines and data processing pipelines, where dividing the text into smaller chunks makes it easier to index and retrieve information.\n",
    "\n",
    "- Context maintenance: When processing involves long documents, this technique allows you to deal with them more efficiently, by dividing the parts without losing logic or cohesion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chunk text separator\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Chunks (Documents): 7\n",
      "Last Chunk Content (Document): page_content='Those who fail to keep up with this natural evolution will be left behind, as we have seen \n",
      "many times throughout human history. Learn as much as you can about different subjects, \n",
      "from interpersonal skills to technical skills. The only limit to what you can learn is the one \n",
      "you impose on yourself. \n",
      "“Be Good at Learning.” Stay in a constant state of learning.' metadata={'source': '../data/article.pdf', 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Applying the object and extracting the chunks (documents)\n",
    "docs = splitter.split_documents(pages)\n",
    "\n",
    "print(\"Total of Chunks (Documents):\", len(docs))\n",
    "\n",
    "print(\"Last Chunk Content (Document):\", docs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading Text Data Vectors into the Vector Database\n",
    "\n",
    "The code implements a semantic search system using a vector database (vectordb) to identify the most relevant points in relation to a question, based on the semantic similarity between the documents and the provided question.\n",
    "\n",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "https://www.trychroma.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krupc\\AppData\\Local\\Temp\\ipykernel_2732\\1485550137.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\"),\n"
     ]
    }
   ],
   "source": [
    "# Create the vector database\n",
    "vectordb = Chroma.from_documents(documents = docs,\n",
    "                                 embedding = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\"),\n",
    "                                 persist_directory = \"vectordb/chroma/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chroma.from_documents(documents=docs)**: Creates a vector database using the provided documents (stored in the docs variable). These documents can be texts, articles, or any type of textual data that you want to index.\n",
    "\n",
    "**HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")**: Uses the Hugging Face all-MiniLM-L6-v2 semantic embedding model to transform texts into numeric vectors. Embeddings are mathematical representations of texts that capture their semantic meaning.\n",
    "\n",
    "**persist_directory=\"dsavectordb/chroma/\"**: Specifies the directory where the vector database will be saved (persisted), so that it can be reused in future sessions without having to reprocess the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total collections in vector db\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Vector Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a question\n",
    "question = \"Has the COVID-19 pandemic accelerated the pace of digital development around the world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 0, 'page_label': '1', 'source': '../data/article.pdf'}, page_content='The Most Important Skill in the Age of Artificial Intelligence \\nThe COVID-19 pandemic accelerated the pace of digital development worldwide, as \\neverything—from meetings to medical consultations—moved online. This might sound \\noverwhelmingly positive. \\nFor tens of millions of workers, it was not. \\nThey may not have the necessary skills to compete in this new world. These are accountants, \\ntypists, and executive secretaries searching for jobs in a new economy where hired candidates \\nhave titles like \"Cloud Engineer\" or \"Growth Hacker\" on their résumés. Without a concerted \\neffort to retrain them, researchers at RAND Europe have found that they are likely to be left \\nbehind. \\nAnd not just them. The cost of this growing skills gap will be measured in trillions of dollars \\nand will hit hardest in places that lack reliable digital infrastructure, such as internet access or \\nwidespread digital literacy. As the global economy struggles to recover from the impact of'), Document(metadata={'page': 0, 'page_label': '1', 'source': '../data/article.pdf'}, page_content=\"Business leaders have been warning for years that what they see on résumés does not match \\nwhat they need in new hires. Europe’s Digital Economy and Society Index recently found \\nthat nearly 60% of employers are struggling to fill digital roles with qualified candidates. Yet, \\nthe realities of the pandemic have left them with no choice: four out of five global business \\nleaders say they are accelerating the automation of processes and daily tasks within their \\ncompanies. \\nThe world's leading economies could now lose $11.5 trillion in potential growth by 2028 if \\nthey fail to close the skills gap, according to the global consulting and professional services \\nfirm Accenture. India, South Africa, and Mexico will be particularly affected. So will groups \\nthat can least afford the economic loss: the elderly, racial and ethnic minorities, and people \\nliving in rural areas. \\nThe World Economic Forum estimates that 85 million jobs could be lost to automation in the\")]\n"
     ]
    }
   ],
   "source": [
    "# Perform the vector search\n",
    "relevant_points = vectordb.max_marginal_relevance_search(question, k = 2, fetch_k = 3)\n",
    "print(relevant_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_marginal_relevance_search()**: Performs a search in the vector database based on maximal marginal relevance (MMR). This technique is used to find documents that are relevant to the given question, reducing redundancy in the answers. Instead of returning documents that are very similar to each other, it ensures diversity in the answers while maintaining relevance. Read the pdf manual in Chapter 16 for more details.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "**question**: The natural text question used to calculate the semantic similarity with the documents in the vector database.\n",
    "\n",
    "**k=2**: Defines the number of final documents that will be returned as the most relevant.\n",
    "\n",
    "**fetch_k=3**: Specifies that the algorithm should initially search for the 3 most relevant documents and then apply the MMR technique to select the 2 most diverse and relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The Most Important Skill in the Age of Artificial Intelligence \n",
      "The COVID-19 pandemic accelerated the pace of digital development worldwide, as \n",
      "everything—from meetings to medical consultations—moved online. This might sound \n",
      "overwhelmingly positive. \n",
      "For tens of millions of workers, it was not. \n",
      "They may not have the necessary skills to compete in this new world. These are accountants, \n",
      "typists, and executive secretaries searching for jobs in a new economy where hired candidates \n",
      "have titles like \"Cloud Engineer\" or \"Growth Hacker\" on their résumés. Without a concerted \n",
      "effort to retrain them, researchers at RAND Europe have found that they are likely to be left \n",
      "behind. \n",
      "And not just them. The cost of this growing skills gap will be measured in trillions of dollars \n",
      "and will hit hardest in places that lack reliable digital infrastructure, such as internet access or \n",
      "widespread digital literacy. As the global economy struggles to recover from the impact of' metadata={'page': 0, 'page_label': '1', 'source': '../data/article.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(relevant_points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LLM\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the LLM as it appears in the HF\n",
    "llm_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model_name, \n",
    "                                             torch_dtype = \"auto\", \n",
    "                                             device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the question\n",
    "question = \"Has the COVID-19 pandemic accelerated the pace of digital development around the world?\"\n",
    "\n",
    "# Extract the context of the question (i.e. perform vector search)\n",
    "context = vectordb.max_marginal_relevance_search(question, k = 2, fetch_k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "prompt = f\"\"\"\n",
    "You are an expert assistant. You use the context provided as your complementary knowledge base to answer the question.\n",
    "context = {context}\n",
    "question = {question}\n",
    "answer =\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of system and user messages\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are an expert assistant.\"},\n",
    "{\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are an expert assistant.<|im_end|>\\n<|im_start|>user\\n\\nYou are an expert assistant. You use the context provided as your complementary knowledge base to answer the question.\\ncontext = [Document(metadata={\\'page\\': 0, \\'page_label\\': \\'1\\', \\'source\\': \\'../data/article.pdf\\'}, page_content=\\'The Most Important Skill in the Age of Artificial Intelligence \\\\nThe COVID-19 pandemic accelerated the pace of digital development worldwide, as \\\\neverything—from meetings to medical consultations—moved online. This might sound \\\\noverwhelmingly positive. \\\\nFor tens of millions of workers, it was not. \\\\nThey may not have the necessary skills to compete in this new world. These are accountants, \\\\ntypists, and executive secretaries searching for jobs in a new economy where hired candidates \\\\nhave titles like \"Cloud Engineer\" or \"Growth Hacker\" on their résumés. Without a concerted \\\\neffort to retrain them, researchers at RAND Europe have found that they are likely to be left \\\\nbehind. \\\\nAnd not just them. The cost of this growing skills gap will be measured in trillions of dollars \\\\nand will hit hardest in places that lack reliable digital infrastructure, such as internet access or \\\\nwidespread digital literacy. As the global economy struggles to recover from the impact of\\'), Document(metadata={\\'page\\': 0, \\'page_label\\': \\'1\\', \\'source\\': \\'../data/article.pdf\\'}, page_content=\"Business leaders have been warning for years that what they see on résumés does not match \\\\nwhat they need in new hires. Europe’s Digital Economy and Society Index recently found \\\\nthat nearly 60% of employers are struggling to fill digital roles with qualified candidates. Yet, \\\\nthe realities of the pandemic have left them with no choice: four out of five global business \\\\nleaders say they are accelerating the automation of processes and daily tasks within their \\\\ncompanies. \\\\nThe world\\'s leading economies could now lose $11.5 trillion in potential growth by 2028 if \\\\nthey fail to close the skills gap, according to the global consulting and professional services \\\\nfirm Accenture. India, South Africa, and Mexico will be particularly affected. So will groups \\\\nthat can least afford the economic loss: the elderly, racial and ethnic minorities, and people \\\\nliving in rural areas. \\\\nThe World Economic Forum estimates that 85 million jobs could be lost to automation in the\")]\\nquestion = Has the COVID-19 pandemic accelerated the pace of digital development around the world?\\nanswer =\\n<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the chat template\n",
    "text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    458,   6203,  17847,\n",
      "             13, 151645,    198, 151644,    872,    271,   2610,    525,    458,\n",
      "           6203,  17847,     13,   1446,    990,    279,   2266,   3897,    438,\n",
      "            697,  57435,   6540,   2331,    311,   4226,    279,   3405,    624,\n",
      "           2147,    284,    508,   7524,  54436,  12854,   2893,   1210,    220,\n",
      "             15,     11,    364,   2893,   6106,   1210,    364,     16,    516,\n",
      "            364,   2427,   1210,   4927,    691,  38181,  15995,  24731,   2150,\n",
      "           7495,   1131,    785,   7496,  43821,  27482,    304,    279,  13081,\n",
      "            315,  58194,  21392,   1124,  88230,  19966,     12,     16,     24,\n",
      "          27422,  48758,    279,  17857,    315,   7377,   4401,  15245,     11,\n",
      "            438,   1124,    811,   1204,   1596,  87858,  16261,    311,   6457,\n",
      "          74437,   2293,  94818,   2860,     13,   1096,   2578,   5112,   1124,\n",
      "             77,   1975,  93079,    398,   6785,     13,   1124,     77,   2461,\n",
      "          22008,    315,  11728,    315,   7337,     11,    432,    572,    537,\n",
      "             13,   1124,     77,   6865,   1231,    537,    614,    279,   5871,\n",
      "           7361,    311,  20259,    304,    419,    501,   1879,     13,   4220,\n",
      "            525,   2692,   1783,     11,   1124,    406,   1082,   1671,     11,\n",
      "            323,  10905,   6234,   5431,  15039,    369,   6887,    304,    264,\n",
      "            501,   8584,   1380,  21446,  11178,   1124,  16719,    523,  15311,\n",
      "           1075,    330,  16055,  28383,      1,    476,    330,     38,  19089,\n",
      "          88065,      1,    389,    862,   9333,   1242,   5397,     13,  17147,\n",
      "            264,  96566,   1124,    811,    542,    371,    311,    312,  10397,\n",
      "           1105,     11,  11811,    518,  72854,   4505,    614,   1730,    429,\n",
      "            807,    525,   4363,    311,    387,   2115,   1124,     77,  29898,\n",
      "            484,     13,   1124,     77,   3036,    537,   1101,   1105,     13,\n",
      "            576,   2783,    315,    419,   7826,   7361,  12929,    686,    387,\n",
      "          16878,    304,    489,  90287,    315,  11192,   1124,     77,    437,\n",
      "            686,   4201,  36454,    304,   7482,    429,   6853,  14720,   7377,\n",
      "          13737,     11,   1741,    438,   7602,   2615,    476,   1124,  61227,\n",
      "           3341,  20717,   7377,  51982,     13,   1634,    279,   3644,   8584,\n",
      "          27870,    311,  11731,    504,    279,   5421,    315,   4567,  11789,\n",
      "          54436,  12854,   2893,   1210,    220,     15,     11,    364,   2893,\n",
      "           6106,   1210,    364,     16,    516,    364,   2427,   1210,   4927,\n",
      "            691,  38181,  15995,  24731,   2150,   7495,    428,  22727,   6036,\n",
      "            614,   1012,   9958,    369,   1635,    429,   1128,    807,   1490,\n",
      "            389,   9333,   1242,   5397,   1558,    537,   2432,   1124,     77,\n",
      "          12555,    807,   1184,    304,    501,  71941,     13,   4505,    748,\n",
      "          14102,  37561,    323,  13278,   8008,   5926,   1730,   1124,     77,\n",
      "           9033,   7009,    220,     21,     15,      4,    315,  22426,    525,\n",
      "          19962,    311,   5155,   7377,  12783,    448,  14988,  11178,     13,\n",
      "          14626,     11,   1124,     77,   1782,  49346,    315,    279,  27422,\n",
      "            614,   2115,   1105,    448,    902,   5754,     25,   3040,    700,\n",
      "            315,   4236,   3644,   2562,   1124,     77,  78386,   1977,    807,\n",
      "            525,  68641,    279,  32662,    315,  11364,    323,   7298,   9079,\n",
      "           2878,    862,   1124,     77,  64751,     13,   1124,  88230,   1879,\n",
      "            594,   6388,  36571,   1410,   1431,   9052,    400,     16,     16,\n",
      "             13,     20,  31510,    304,   4650,   6513,    553,    220,     17,\n",
      "             15,     17,     23,    421,   1124,     77,  20069,   3690,    311,\n",
      "           3265,    279,   7361,  12929,     11,   4092,    311,    279,   3644,\n",
      "          30731,    323,   6584,   3516,   1124,     77,   8802,  81809,    552,\n",
      "             13,   6747,     11,   4882,  10174,     11,    323,  12270,    686,\n",
      "            387,   7945,  11495,     13,   2055,    686,   5203,   1124,     77,\n",
      "           9033,    646,   3245,   9946,    279,   6955,   4709,     25,    279,\n",
      "          28820,     11,  19189,    323,  21551,  39704,     11,    323,   1251,\n",
      "           1124,  15643,   2249,    304,  19082,   5671,     13,   1124,  88230,\n",
      "           4337,  22546,  17538,  17530,    429,    220,     23,     20,   3526,\n",
      "           6887,   1410,    387,   5558,    311,  32662,    304,    279,   5422,\n",
      "           7841,    284,  11443,    279,  19966,     12,     16,     24,  27422,\n",
      "          48758,    279,  17857,    315,   7377,   4401,   2163,    279,   1879,\n",
      "           5267,   9217,   4035, 151645,    198, 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization\n",
    "model_inputs = tokenizer([text], return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Answers with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    458,   6203,  17847,\n",
      "             13, 151645,    198, 151644,    872,    271,   2610,    525,    458,\n",
      "           6203,  17847,     13,   1446,    990,    279,   2266,   3897,    438,\n",
      "            697,  57435,   6540,   2331,    311,   4226,    279,   3405,    624,\n",
      "           2147,    284,    508,   7524,  54436,  12854,   2893,   1210,    220,\n",
      "             15,     11,    364,   2893,   6106,   1210,    364,     16,    516,\n",
      "            364,   2427,   1210,   4927,    691,  38181,  15995,  24731,   2150,\n",
      "           7495,   1131,    785,   7496,  43821,  27482,    304,    279,  13081,\n",
      "            315,  58194,  21392,   1124,  88230,  19966,     12,     16,     24,\n",
      "          27422,  48758,    279,  17857,    315,   7377,   4401,  15245,     11,\n",
      "            438,   1124,    811,   1204,   1596,  87858,  16261,    311,   6457,\n",
      "          74437,   2293,  94818,   2860,     13,   1096,   2578,   5112,   1124,\n",
      "             77,   1975,  93079,    398,   6785,     13,   1124,     77,   2461,\n",
      "          22008,    315,  11728,    315,   7337,     11,    432,    572,    537,\n",
      "             13,   1124,     77,   6865,   1231,    537,    614,    279,   5871,\n",
      "           7361,    311,  20259,    304,    419,    501,   1879,     13,   4220,\n",
      "            525,   2692,   1783,     11,   1124,    406,   1082,   1671,     11,\n",
      "            323,  10905,   6234,   5431,  15039,    369,   6887,    304,    264,\n",
      "            501,   8584,   1380,  21446,  11178,   1124,  16719,    523,  15311,\n",
      "           1075,    330,  16055,  28383,      1,    476,    330,     38,  19089,\n",
      "          88065,      1,    389,    862,   9333,   1242,   5397,     13,  17147,\n",
      "            264,  96566,   1124,    811,    542,    371,    311,    312,  10397,\n",
      "           1105,     11,  11811,    518,  72854,   4505,    614,   1730,    429,\n",
      "            807,    525,   4363,    311,    387,   2115,   1124,     77,  29898,\n",
      "            484,     13,   1124,     77,   3036,    537,   1101,   1105,     13,\n",
      "            576,   2783,    315,    419,   7826,   7361,  12929,    686,    387,\n",
      "          16878,    304,    489,  90287,    315,  11192,   1124,     77,    437,\n",
      "            686,   4201,  36454,    304,   7482,    429,   6853,  14720,   7377,\n",
      "          13737,     11,   1741,    438,   7602,   2615,    476,   1124,  61227,\n",
      "           3341,  20717,   7377,  51982,     13,   1634,    279,   3644,   8584,\n",
      "          27870,    311,  11731,    504,    279,   5421,    315,   4567,  11789,\n",
      "          54436,  12854,   2893,   1210,    220,     15,     11,    364,   2893,\n",
      "           6106,   1210,    364,     16,    516,    364,   2427,   1210,   4927,\n",
      "            691,  38181,  15995,  24731,   2150,   7495,    428,  22727,   6036,\n",
      "            614,   1012,   9958,    369,   1635,    429,   1128,    807,   1490,\n",
      "            389,   9333,   1242,   5397,   1558,    537,   2432,   1124,     77,\n",
      "          12555,    807,   1184,    304,    501,  71941,     13,   4505,    748,\n",
      "          14102,  37561,    323,  13278,   8008,   5926,   1730,   1124,     77,\n",
      "           9033,   7009,    220,     21,     15,      4,    315,  22426,    525,\n",
      "          19962,    311,   5155,   7377,  12783,    448,  14988,  11178,     13,\n",
      "          14626,     11,   1124,     77,   1782,  49346,    315,    279,  27422,\n",
      "            614,   2115,   1105,    448,    902,   5754,     25,   3040,    700,\n",
      "            315,   4236,   3644,   2562,   1124,     77,  78386,   1977,    807,\n",
      "            525,  68641,    279,  32662,    315,  11364,    323,   7298,   9079,\n",
      "           2878,    862,   1124,     77,  64751,     13,   1124,  88230,   1879,\n",
      "            594,   6388,  36571,   1410,   1431,   9052,    400,     16,     16,\n",
      "             13,     20,  31510,    304,   4650,   6513,    553,    220,     17,\n",
      "             15,     17,     23,    421,   1124,     77,  20069,   3690,    311,\n",
      "           3265,    279,   7361,  12929,     11,   4092,    311,    279,   3644,\n",
      "          30731,    323,   6584,   3516,   1124,     77,   8802,  81809,    552,\n",
      "             13,   6747,     11,   4882,  10174,     11,    323,  12270,    686,\n",
      "            387,   7945,  11495,     13,   2055,    686,   5203,   1124,     77,\n",
      "           9033,    646,   3245,   9946,    279,   6955,   4709,     25,    279,\n",
      "          28820,     11,  19189,    323,  21551,  39704,     11,    323,   1251,\n",
      "           1124,  15643,   2249,    304,  19082,   5671,     13,   1124,  88230,\n",
      "           4337,  22546,  17538,  17530,    429,    220,     23,     20,   3526,\n",
      "           6887,   1410,    387,   5558,    311,  32662,    304,    279,   5422,\n",
      "           7841,    284,  11443,    279,  19966,     12,     16,     24,  27422,\n",
      "          48758,    279,  17857,    315,   7377,   4401,   2163,    279,   1879,\n",
      "           5267,   9217,   4035, 151645,    198, 151644,  77091,    198,   9454,\n",
      "             11,    279,  19966,     12,     16,     24,  27422,    702,  12824,\n",
      "          48758,    279,  17857,    315,   7377,   4401,  30450,     13,   1096,\n",
      "            374,  29476,    504,   3807,   3501,    304,    279,   2661,   2266,\n",
      "           1447,     16,     13,   3070,  24841,    311,   8105,  95518,    576,\n",
      "           2197,  33845,    429,   4297,  87858,  16261,    311,   6457,  74437,\n",
      "           2293,  94818,   2860,   4152,    311,    279,  27422,     13,   1096,\n",
      "           6407,  14807,    264,   5089,  30803,    304,    279,  24376,    323,\n",
      "          17590,    315,   7377,  14310,   1119,   5257,  13566,    315,   2272,\n",
      "            382,     17,     13,   3070,  71503,    389,  35698,  95518,   1084,\n",
      "           8388,    429,   1657,   7337,    879,   1033,   8597,  11889,    311,\n",
      "          20259,    304,    264,    501,   8584,    448,  15311,   1075,    330,\n",
      "          16055,  28383,      1,    476,    330,     38,  19089,  88065,      1,\n",
      "            389,    862,  65213,   1033,   2115,   4815,     13,   1096,  13230,\n",
      "            429,    279,   8606,  30927,    374,   1660,  68527,    553,    279,\n",
      "          10000,    315,    501,   2618,   8502,  17339,    311,   7377,   7361,\n",
      "            382,     18,     13,   3070,  46661,  57915,  95518,    576,   2266,\n",
      "          21314,    429,   1052,    374,    264,   7826,   7361,  12929,   1948,\n",
      "           1128,   9675,    525,   3330,    369,    304,   8256,    320,  60817,\n",
      "           5435,    311,   7377,   7361,      8,    323,   1128,    279,   1482,\n",
      "          30927,  49226,     13,   1096,  12929,    374,  91441,    553,    279,\n",
      "          27422,  37873,   4344,    304,   1246,    975,    374,  13075,    382,\n",
      "             19,     13,   3070,     36,  31007,  28580,  95518,    576,   1467,\n",
      "          34334,    279,   4650,   1293,   9663,   7049,   5815,    448,    419,\n",
      "           7361,  12929,     11,  76372,    264,   4709,    315,    489,  90287,\n",
      "            315,  11192,    323,  22561,    429,   5837,  31061,  14720,   7377,\n",
      "          13737,   1035,   7676,  72052,     13,  22406,     11,    432,  33845,\n",
      "           3151,  13604,   1741,    438,   6747,     11,   4882,  10174,     11,\n",
      "            323,  12270,     11,    892,    525,  26366,    438,   1660,   7945,\n",
      "          19563,    311,   1493,   6239,    382,  27489,     11,    279,  11048,\n",
      "          17991,   7117,    911,    553,    279,  27422,  71790,   1181,   3476,\n",
      "            304,  68641,    279,  17857,    315,   7377,   4401,  30450,     13,\n",
      "         151645]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens = 512)\n",
    "\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  9454,     11,    279,  19966,     12,     16,     24,  27422,    702,\n",
      "         12824,  48758,    279,  17857,    315,   7377,   4401,  30450,     13,\n",
      "          1096,    374,  29476,    504,   3807,   3501,    304,    279,   2661,\n",
      "          2266,   1447,     16,     13,   3070,  24841,    311,   8105,  95518,\n",
      "           576,   2197,  33845,    429,   4297,  87858,  16261,    311,   6457,\n",
      "         74437,   2293,  94818,   2860,   4152,    311,    279,  27422,     13,\n",
      "          1096,   6407,  14807,    264,   5089,  30803,    304,    279,  24376,\n",
      "           323,  17590,    315,   7377,  14310,   1119,   5257,  13566,    315,\n",
      "          2272,    382,     17,     13,   3070,  71503,    389,  35698,  95518,\n",
      "          1084,   8388,    429,   1657,   7337,    879,   1033,   8597,  11889,\n",
      "           311,  20259,    304,    264,    501,   8584,    448,  15311,   1075,\n",
      "           330,  16055,  28383,      1,    476,    330,     38,  19089,  88065,\n",
      "             1,    389,    862,  65213,   1033,   2115,   4815,     13,   1096,\n",
      "         13230,    429,    279,   8606,  30927,    374,   1660,  68527,    553,\n",
      "           279,  10000,    315,    501,   2618,   8502,  17339,    311,   7377,\n",
      "          7361,    382,     18,     13,   3070,  46661,  57915,  95518,    576,\n",
      "          2266,  21314,    429,   1052,    374,    264,   7826,   7361,  12929,\n",
      "          1948,   1128,   9675,    525,   3330,    369,    304,   8256,    320,\n",
      "         60817,   5435,    311,   7377,   7361,      8,    323,   1128,    279,\n",
      "          1482,  30927,  49226,     13,   1096,  12929,    374,  91441,    553,\n",
      "           279,  27422,  37873,   4344,    304,   1246,    975,    374,  13075,\n",
      "           382,     19,     13,   3070,     36,  31007,  28580,  95518,    576,\n",
      "          1467,  34334,    279,   4650,   1293,   9663,   7049,   5815,    448,\n",
      "           419,   7361,  12929,     11,  76372,    264,   4709,    315,    489,\n",
      "         90287,    315,  11192,    323,  22561,    429,   5837,  31061,  14720,\n",
      "          7377,  13737,   1035,   7676,  72052,     13,  22406,     11,    432,\n",
      "         33845,   3151,  13604,   1741,    438,   6747,     11,   4882,  10174,\n",
      "            11,    323,  12270,     11,    892,    525,  26366,    438,   1660,\n",
      "          7945,  19563,    311,   1493,   6239,    382,  27489,     11,    279,\n",
      "         11048,  17991,   7117,    911,    553,    279,  27422,  71790,   1181,\n",
      "          3476,    304,  68641,    279,  17857,    315,   7377,   4401,  30450,\n",
      "            13, 151645], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# Unpack the responses\n",
    "# Goal: Extract only the tokens generated by the model (i.e. the part of the output that comes after\n",
    "# the input tokens). This is useful because models like GPT or others based on autoregressive\n",
    "# decoding often return a concatenation of the input and output.\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the COVID-19 pandemic has indeed accelerated the pace of digital development globally. This is evident from several points in the given context:\n",
      "\n",
      "1. **Shift to Online**: The document mentions that everything—from meetings to medical consultations—moved online due to the pandemic. This shift indicates a significant acceleration in the adoption and integration of digital technologies into various aspects of life.\n",
      "\n",
      "2. **Impact on Workers**: It notes that many workers who were previously unable to compete in a new economy with titles like \"Cloud Engineer\" or \"Growth Hacker\" on their resumes were left behind. This suggests that the traditional workforce is being disrupted by the rise of new job requirements tied to digital skills.\n",
      "\n",
      "3. **Skills Gap**: The context highlights that there is a growing skills gap between what businesses are looking for in employees (often related to digital skills) and what the current workforce possesses. This gap is exacerbated by the pandemic-induced changes in how work is conducted.\n",
      "\n",
      "4. **Economic Impact**: The text discusses the potential long-term costs associated with this skills gap, estimating a loss of trillions of dollars and suggesting that countries lacking reliable digital infrastructure would suffer disproportionately. Additionally, it mentions specific regions such as India, South Africa, and Mexico, which are highlighted as being particularly vulnerable to these effects.\n",
      "\n",
      "Overall, the rapid transformation brought about by the pandemic underscores its role in accelerating the pace of digital development globally.\n"
     ]
    }
   ],
   "source": [
    "# Apply the decode to get the generated text\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Answer System using Our Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the information provided in the context, the World Economic Forum estimates that 85 million jobs could be lost to automation in the next three years across more than a dozen industries.\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"How many jobs does the World Economic Forum estimate will be lost to automation in the coming years?\"\n",
    "\n",
    "# Extract the context from the question (i.e. perform vector search)\n",
    "context = vectordb.max_marginal_relevance_search(question, k = 2, fetch_k = 3)\n",
    "\n",
    "# Create the prompt\n",
    "prompt = f\"\"\"\n",
    "You are an expert assistant. You use the provided context as your supplemental knowledge base to answer the question.\n",
    "context = {context}\n",
    "question = {question}\n",
    "answer =\n",
    "\"\"\"\n",
    "\n",
    "# Create the list of system and user messages\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are an expert assistant.\"},\n",
    "{\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "# Apply tokenization\n",
    "model_inputs = tokenizer([text], return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "# Generate response with LLM\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens = 512)\n",
    "\n",
    "# Unpack the answers\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "# Apply the decode to obtain the generated text\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
