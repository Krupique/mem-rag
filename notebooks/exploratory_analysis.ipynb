{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\Downloads\\Projects\\mlops\\mem-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "import chromadb\n",
    "import urllib3\n",
    "import accelerate\n",
    "import sentence_transformers\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=True\n",
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=True\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text data from PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.document_loaders.pdf.PyPDFLoader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an object to load PDF file\n",
    "loader = PyPDFLoader('../data/ArtigoDSA1.pdf')\n",
    "\n",
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/ArtigoDSA1.pdf', 'page': 0, 'page_label': '1'}, page_content='A Habilidade Mais Importante na Era da Inteligência Artificial \\n \\nA pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo, já que \\ntudo, desde reuniões até consultas médicas, ficou online. Isso pode soar como algo super \\npositivo.  \\nPara dezenas de milhões de trabalhadores, não. \\nEles talvez não tenham as habilidades necessárias para competir nesse novo mundo. Eles são os \\ncontadores, os digitadores, os secretários executivos, procurando trabalho em uma nova \\neconomia na qual as pessoas contratadas têm títulos como “Engenheiro de Nuv em” ou “Hacker \\nde Crescimento” em seus currículos. Sem um esforço concentrado para retreiná-los, descobriram \\nos pesquisadores da RAND Europe, eles provavelmente serão deixados para trás. \\nE não apenas eles. O custo dessa crescente lacuna de habilidades será medido em trilhões de \\ndólares e recairá mais fortemente em lugares que não possuem infraestrutura digital confiável, \\ncomo acesso à Internet ou fluência generalizada em habilidades digita is. À medida que a \\neconomia mundial luta para se levantar após o golpe do COVID -19, essa lacuna de habilidades \\nameaça continuar pressionando para baixo. \\n“Simplesmente não há pessoas suficientes com as habilidades digitais certas para permitir a \\ntransformação que as empresas estão buscando”, disse Salil Gunashekar, líder de pesquisa e \\ndiretor associado da RAND Europe, que se concentra na política de ciência e tecnologia. \\nEm algum momento nos próximos anos, o mundo passará por um marco importante. O número \\nde horas trabalhadas pelas máquinas será igual ao número de horas trabalhadas pelos humanos. \\nUma pesquisa recente da Salesforce descobriu que três quartos dos trabalhadores do mundo se \\nsentem despreparados para os empregos que podem encontrar do outro lado desse marco. \\nAqueles que planejam trabalhar em assistência médica ou serviços financeiros, por exemplo, \\npodem precisar saber como usar computadores com Inteligência Artificial. Aqueles que desejam \\ntrabalhar em mineração de metais podem precisar saber como operar robôs e analisar Big Data. \\nUm contador pode ser tornar o operador de um robô de automação de processos. \\nOs líderes empresariais alertam há anos que o que veem nos currículos não corresponde ao que \\nprecisam em novos funcionários. O Índice de Economia e Sociedade Digital da Europa descobriu \\nrecentemente que quase 60% dos empregadores estão tendo problemas para  preencher vagas \\ndigitais com candidatos qualificados. E, no entanto, as realidades pandêmicas não os deixaram \\nescolha: quatro em cada cinco líderes empresariais globais dizem que estão acelerando a \\nautomação de processos e tarefas do dia a dia dentro da empresa. '),\n",
       " Document(metadata={'source': '../data/ArtigoDSA1.pdf', 'page': 1, 'page_label': '2'}, page_content='As principais economias do mundo agora podem perder US$ 11,5 trilhões em crescimento \\npotencial até 2028 se não conseguirem preencher a lacuna de habilidades, estimou a empresa \\nglobal de consultoria e serviços profissionais Accenture. Índia, África do Sul e  México serão \\nespecialmente atingidos. O mesmo acontecerá com os grupos que menos podem arcar com a \\nperda econômica: idosos, minorias raciais e étnicas e pessoas que vivem em áreas rurais. \\nO Fórum Econômico Mundial estima que 85 milhões de empregos podem ser perdidos para a \\nautomação nos próximos três anos em mais de uma dúzia de setores. Ao mesmo tempo, espera \\nque surjam 97 milhões de novos empregos melhor adaptados ao futuro do trabalho. N o papel, \\nisso deve ser uma vitória. Sem um grande compromisso para reter e retreinar os trabalhadores \\nexistentes, descobriu a RAND Europe, será uma perda para os funcionários e uma perda para os \\nempregadores. \\nNão há soluções simples aqui. As empresas precisam se tornar mais ágeis na distribuição e \\nredistribuição de seus funcionários existentes para melhor atender às suas necessidades, em vez \\nde tentar recrutar para sair da lacuna de habilidades. Elas também pre cisam fazer mais para \\najudar esses funcionários a aprender as habilidades técnicas, como programação e análise de \\ndados, e as habilidades interpessoais, como trabalhar em equipe, de que precisam para ter \\nsucesso. Os governos nacionais podem ajudar investin do em programas vocacionais e outros \\napoios para trabalhadores desalocados. \\nUm passo importante seria desenvolver uma “linguagem de habilidades” comum, escreveram os \\npesquisadores. Isso garantiria que candidatos e empregadores tivessem a mesma intenção ao \\nusar um termo como “Engenheiro de Nuvem” ou “Engenheiro de IA”. Isso ajudari a os gerentes \\nde contratação a avaliar rapidamente os candidatos com base nas habilidades que eles trazem \\npara o trabalho e não apenas no nome da faculdade em seu currículo (que no mundo atual já não \\ntem mais qualquer relevância). \\nOs trabalhadores, entretanto, precisam mudar sua mentalidade. A educação não termina mais \\ncom um diploma do ensino médio ou um diploma universitário. As habilidades que eles têm \\nagora podem não ser relevantes em alguns anos. Como aconselhou um gerente de tecnologia no \\nCanadá entrevistado durante a pesquisa: “Seja Bom em Aprender”. \\nSim. Esta é a habilidade mais importante na era da Inteligência Artificial:  \\n“Seja Bom em Aprender”. \\n \\nA transformação digital requer que você aprenda, desaprenda, reaprenda e permaneça nesse \\nciclo se deseja realmente manter sua empregabilidade. A capacidade de adaptação a novas \\ntecnologias e a habilidade em aprender cada vez mais rápido, é o que vai diferenciar você das \\nmáquinas. '),\n",
       " Document(metadata={'source': '../data/ArtigoDSA1.pdf', 'page': 2, 'page_label': '3'}, page_content='Não importa sua área, seu mercado, sua graduação, sua idade ou seu gênero. O mundo está \\npassando por uma profunda transformação digital e os empregos como conhecemos estão sendo \\nreinventados. Aqueles que não acompanharem essa evolução natural ficarão para trás, como \\ntantas vezes vimos na história humana. Aprenda o máximo que puder, sobre diferentes temas, \\ndesde habilidades interpessoais até habilidades técnicas. O único limite sobre o que você pode \\naprender é o que você impõe a si mesmo. \\n“Seja Bom em Aprender”. Mantenha-se em modo constante de aprendizado. \\nEquipe DSA \\nwww.datascienceacademy.com.br \\n ')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the file PDF\n",
    "pages = loader.load()\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content:  Não importa sua área, seu mercado, sua graduação, sua idade ou seu gênero. O mundo está \n",
      "passando por uma profunda transformação digital e os empregos como conhecemos estão sendo \n",
      "reinventados. Aqueles que não acompanharem essa evolução natural ficarão para trás, como \n",
      "tantas vezes vimos na história humana. Aprenda o máximo que puder, sobre diferentes temas, \n",
      "desde habilidades interpessoais até habilidades técnicas. O único limite sobre o que você pode \n",
      "aprender é o que você impõe a si mesmo. \n",
      "“\n"
     ]
    }
   ],
   "source": [
    "page = pages[2]\n",
    "\n",
    "print(\"Page content: \", page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': '../data/ArtigoDSA1.pdf', 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Metadata:\", page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text Data in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chunk_size = 1000**: Specifies that each resulting chunk of text will have a maximum of 1000 characters.\n",
    "\n",
    "**chunk_overlap = 20**: Indicates that each chunk will have 20 characters of overlap with the next chunk. This means that the last 20 characters of a chunk will be repeated at the beginning of the next chunk.\n",
    "\n",
    "What it is for:\n",
    "\n",
    "This approach is useful in several situations where large texts need to be processed or analyzed, such as:\n",
    "\n",
    "- Input for language models: Many LLMs have a limit of tokens that they can process in a single iteration. Dividing the text into smaller chunks ensures that the text is sent within the allowed limit.\n",
    "\n",
    "- Data analysis and indexing: It is common in search engines and data processing pipelines, where dividing the text into smaller chunks makes it easier to index and retrieve information.\n",
    "\n",
    "- Context maintenance: When processing involves long documents, this technique allows you to deal with them more efficiently, by dividing the parts without losing logic or cohesion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chunk text separator\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Chunks (Documents): 7\n",
      "Last Chunk Content (Document): page_content='Não importa sua área, seu mercado, sua graduação, sua idade ou seu gênero. O mundo está \n",
      "passando por uma profunda transformação digital e os empregos como conhecemos estão sendo \n",
      "reinventados. Aqueles que não acompanharem essa evolução natural ficarão para trás, como \n",
      "tantas vezes vimos na história humana. Aprenda o máximo que puder, sobre diferentes temas, \n",
      "desde habilidades interpessoais até habilidades técnicas. O único limite sobre o que você pode \n",
      "aprender é o que você impõe a si mesmo. \n",
      "“Seja Bom em Aprender”. Mantenha-se em modo constante de aprendizado. \n",
      "Equipe DSA \n",
      "www.datascienceacademy.com.br' metadata={'source': '../data/ArtigoDSA1.pdf', 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "# Applying the object and extracting the chunks (documents)\n",
    "docs = splitter.split_documents(pages)\n",
    "\n",
    "print(\"Total of Chunks (Documents):\", len(docs))\n",
    "\n",
    "print(\"Last Chunk Content (Document):\", docs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading Text Data Vectors into the Vector Database\n",
    "\n",
    "The code implements a semantic search system using a vector database (vectordb) to identify the most relevant points in relation to a question, based on the semantic similarity between the documents and the provided question.\n",
    "\n",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "https://www.trychroma.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krupc\\AppData\\Local\\Temp\\ipykernel_4952\\1485550137.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\"),\n"
     ]
    }
   ],
   "source": [
    "# Create the vector database\n",
    "vectordb = Chroma.from_documents(documents = docs,\n",
    "                                 embedding = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\"),\n",
    "                                 persist_directory = \"vectordb/chroma/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chroma.from_documents(documents=docs)**: Creates a vector database using the provided documents (stored in the docs variable). These documents can be texts, articles, or any type of textual data that you want to index.\n",
    "\n",
    "**HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")**: Uses the Hugging Face all-MiniLM-L6-v2 semantic embedding model to transform texts into numeric vectors. Embeddings are mathematical representations of texts that capture their semantic meaning.\n",
    "\n",
    "**persist_directory=\"dsavectordb/chroma/\"**: Specifies the directory where the vector database will be saved (persisted), so that it can be reused in future sessions without having to reprocess the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total collections in vector db\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Vector Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a question\n",
    "question = \"Has the COVID-19 pandemic accelerated the pace of digital development around the world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 0, 'page_label': '1', 'source': '../data/ArtigoDSA1.pdf'}, page_content='A Habilidade Mais Importante na Era da Inteligência Artificial \\n \\nA pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo, já que \\ntudo, desde reuniões até consultas médicas, ficou online. Isso pode soar como algo super \\npositivo.  \\nPara dezenas de milhões de trabalhadores, não. \\nEles talvez não tenham as habilidades necessárias para competir nesse novo mundo. Eles são os \\ncontadores, os digitadores, os secretários executivos, procurando trabalho em uma nova \\neconomia na qual as pessoas contratadas têm títulos como “Engenheiro de Nuv em” ou “Hacker \\nde Crescimento” em seus currículos. Sem um esforço concentrado para retreiná-los, descobriram \\nos pesquisadores da RAND Europe, eles provavelmente serão deixados para trás. \\nE não apenas eles. O custo dessa crescente lacuna de habilidades será medido em trilhões de \\ndólares e recairá mais fortemente em lugares que não possuem infraestrutura digital confiável,'), Document(metadata={'page': 0, 'page_label': '1', 'source': '../data/ArtigoDSA1.pdf'}, page_content='A Habilidade Mais Importante na Era da Inteligência Artificial \\n \\nA pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo, já que \\ntudo, desde reuniões até consultas médicas, ficou online. Isso pode soar como algo super \\npositivo.  \\nPara dezenas de milhões de trabalhadores, não. \\nEles talvez não tenham as habilidades necessárias para competir nesse novo mundo. Eles são os \\ncontadores, os digitadores, os secretários executivos, procurando trabalho em uma nova \\neconomia na qual as pessoas contratadas têm títulos como “Engenheiro de Nuv em” ou “Hacker \\nde Crescimento” em seus currículos. Sem um esforço concentrado para retreiná-los, descobriram \\nos pesquisadores da RAND Europe, eles provavelmente serão deixados para trás. \\nE não apenas eles. O custo dessa crescente lacuna de habilidades será medido em trilhões de \\ndólares e recairá mais fortemente em lugares que não possuem infraestrutura digital confiável,')]\n"
     ]
    }
   ],
   "source": [
    "# Perform the vector search\n",
    "relevant_points = vectordb.max_marginal_relevance_search(question, k = 2, fetch_k = 3)\n",
    "print(relevant_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_marginal_relevance_search()**: Performs a search in the vector database based on maximal marginal relevance (MMR). This technique is used to find documents that are relevant to the given question, reducing redundancy in the answers. Instead of returning documents that are very similar to each other, it ensures diversity in the answers while maintaining relevance. Read the pdf manual in Chapter 16 for more details.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "**question**: The natural text question used to calculate the semantic similarity with the documents in the vector database.\n",
    "\n",
    "**k=2**: Defines the number of final documents that will be returned as the most relevant.\n",
    "\n",
    "**fetch_k=3**: Specifies that the algorithm should initially search for the 3 most relevant documents and then apply the MMR technique to select the 2 most diverse and relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A Habilidade Mais Importante na Era da Inteligência Artificial \n",
      " \n",
      "A pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo, já que \n",
      "tudo, desde reuniões até consultas médicas, ficou online. Isso pode soar como algo super \n",
      "positivo.  \n",
      "Para dezenas de milhões de trabalhadores, não. \n",
      "Eles talvez não tenham as habilidades necessárias para competir nesse novo mundo. Eles são os \n",
      "contadores, os digitadores, os secretários executivos, procurando trabalho em uma nova \n",
      "economia na qual as pessoas contratadas têm títulos como “Engenheiro de Nuv em” ou “Hacker \n",
      "de Crescimento” em seus currículos. Sem um esforço concentrado para retreiná-los, descobriram \n",
      "os pesquisadores da RAND Europe, eles provavelmente serão deixados para trás. \n",
      "E não apenas eles. O custo dessa crescente lacuna de habilidades será medido em trilhões de \n",
      "dólares e recairá mais fortemente em lugares que não possuem infraestrutura digital confiável,' metadata={'page': 0, 'page_label': '1', 'source': '../data/ArtigoDSA1.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(relevant_points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LLM\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the LLM as it appears in the HF\n",
    "llm_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model_name, \n",
    "                                             torch_dtype = \"auto\", \n",
    "                                             device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the question\n",
    "question = \"Has the COVID-19 pandemic accelerated the pace of digital development around the world?\"\n",
    "\n",
    "# Extract the context of the question (i.e. perform vector search)\n",
    "context = vectordb.max_marginal_relevance_search(question, k = 2, fetch_k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "prompt = f\"\"\"\n",
    "You are an expert assistant. You use the context provided as your complementary knowledge base to answer the question.\n",
    "context = {context}\n",
    "question = {question}\n",
    "answer =\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of system and user messages\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are an expert assistant.\"},\n",
    "{\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are an expert assistant.<|im_end|>\\n<|im_start|>user\\n\\nYou are an expert assistant. You use the context provided as your complementary knowledge base to answer the question.\\ncontext = [Document(metadata={'page': 0, 'page_label': '1', 'source': '../data/ArtigoDSA1.pdf'}, page_content='A Habilidade Mais Importante na Era da Inteligência Artificial \\\\n \\\\nA pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo, já que \\\\ntudo, desde reuniões até consultas médicas, ficou online. Isso pode soar como algo super \\\\npositivo.  \\\\nPara dezenas de milhões de trabalhadores, não. \\\\nEles talvez não tenham as habilidades necessárias para competir nesse novo mundo. Eles são os \\\\ncontadores, os digitadores, os secretários executivos, procurando trabalho em uma nova \\\\neconomia na qual as pessoas contratadas têm títulos como “Engenheiro de Nuv em” ou “Hacker \\\\nde Crescimento” em seus currículos. Sem um esforço concentrado para retreiná-los, descobriram \\\\nos pesquisadores da RAND Europe, eles provavelmente serão deixados para trás. \\\\nE não apenas eles. O custo dessa crescente lacuna de habilidades será medido em trilhões de \\\\ndólares e recairá mais fortemente em lugares que não possuem infraestrutura digital confiável,'), Document(metadata={'page': 0, 'page_label': '1', 'source': '../data/ArtigoDSA1.pdf'}, page_content='A Habilidade Mais Importante na Era da Inteligência Artificial \\\\n \\\\nA pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo, já que \\\\ntudo, desde reuniões até consultas médicas, ficou online. Isso pode soar como algo super \\\\npositivo.  \\\\nPara dezenas de milhões de trabalhadores, não. \\\\nEles talvez não tenham as habilidades necessárias para competir nesse novo mundo. Eles são os \\\\ncontadores, os digitadores, os secretários executivos, procurando trabalho em uma nova \\\\neconomia na qual as pessoas contratadas têm títulos como “Engenheiro de Nuv em” ou “Hacker \\\\nde Crescimento” em seus currículos. Sem um esforço concentrado para retreiná-los, descobriram \\\\nos pesquisadores da RAND Europe, eles provavelmente serão deixados para trás. \\\\nE não apenas eles. O custo dessa crescente lacuna de habilidades será medido em trilhões de \\\\ndólares e recairá mais fortemente em lugares que não possuem infraestrutura digital confiável,')]\\nquestion = Has the COVID-19 pandemic accelerated the pace of digital development around the world?\\nanswer =\\n<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the chat template\n",
    "text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    458,   6203,  17847,\n",
      "             13, 151645,    198, 151644,    872,    271,   2610,    525,    458,\n",
      "           6203,  17847,     13,   1446,    990,    279,   2266,   3897,    438,\n",
      "            697,  57435,   6540,   2331,    311,   4226,    279,   3405,    624,\n",
      "           2147,    284,    508,   7524,  54436,  12854,   2893,   1210,    220,\n",
      "             15,     11,    364,   2893,   6106,   1210,    364,     16,    516,\n",
      "            364,   2427,   1210,   4927,    691,     14,   9286,   7836,  72638,\n",
      "             16,  15995,  24731,   2150,   7495,   1131,     32,    472,  79916,\n",
      "          33347,  13213,   4942,   4317,  47588,   2994,  15611,    343,  23696,\n",
      "          58194,   1124,     77,   1124,     77,     32,  12217,  21925,    653,\n",
      "          19966,     12,     16,     24,   1613,   7865,    283,    297,  21198,\n",
      "           6355,    653,  60244,  78669,   7377,    976,  11804,    297,  28352,\n",
      "             11,  32092,   1709,   1124,    406,   7680,     11,  22718,    312,\n",
      "          15705,  12652,  38483,   8498,    300,  33930,  15185,     11,  41255,\n",
      "            283,   2860,     13,   2160,    704,  28194,  98465,   7953,  27928,\n",
      "           2256,   1124,     77,   2724,   6496,     13,    220,   1124,     77,\n",
      "          30205,    409,   5679,    300,    409, 133893,    409,  36268,     71,\n",
      "          18244,     11,  12393,     13,   1124,     77,     36,    642,   8210,\n",
      "          19069,  12393,   5779,   5604,    438,  94215,  13596,   4441,   1953,\n",
      "          47831,   3348,   4533,    404,    308,  23318,  38323,  28352,     13,\n",
      "            468,    642,  29610,   2643,   1124,     77,    772,  18244,     11,\n",
      "           2643,  15723,  18244,     11,   2643,   6234,  37085,  23494,  24224,\n",
      "             11,  70502,   4883,  54639,    976,  10608,  40534,   1124,    811,\n",
      "          44217,    685,   4317,   5841,    438,  45962,  87003,  11107,  98860,\n",
      "            259,  16095,  28652,   7953,   1036,   4106,    268,  59477,    409,\n",
      "            451,  12058,    976,    854,   5908,   1036,     39,   9683,   1124,\n",
      "          42341,  60910,     66,  15027,    854,    976,  41398,   9804,  66011,\n",
      "             13,  14248,   4443,   1531,   1958,  20210,  17137,   2123,   3348,\n",
      "           2112,    265,    258,   1953,     12,   2301,     11,   6560,    674,\n",
      "          37215,    309,   1124,  36391,  18050,   9202,  18244,   2994,  72854,\n",
      "           4505,     11,  66441,   2543,   3878,  12541,  98424,  64756,   5553,\n",
      "           3348,    489,   7061,     13,   1124,     77,     36,  12393,  46667,\n",
      "          66441,     13,    506,  16564,     78,  85479,  45758,  77584,  43879,\n",
      "           8565,    409,  94215,  13596,  32898,   1774,   5249,    976,    489,\n",
      "            321,  93230,    409,   1124,    303,   1794,   4260,    416,    384,\n",
      "            312,    924,  83562,   9870,    369,    870,   6817,    976,  92824,\n",
      "           1709,  12393,   2229,  69847,  48176,  15111,  78707,   7377,   2335,\n",
      "             72,  43315,   2894,    701,  11789,  54436,  12854,   2893,   1210,\n",
      "            220,     15,     11,    364,   2893,   6106,   1210,    364,     16,\n",
      "            516,    364,   2427,   1210,   4927,    691,     14,   9286,   7836,\n",
      "          72638,     16,  15995,  24731,   2150,   7495,   1131,     32,    472,\n",
      "          79916,  33347,  13213,   4942,   4317,  47588,   2994,  15611,    343,\n",
      "          23696,  58194,   1124,     77,   1124,     77,     32,  12217,  21925,\n",
      "            653,  19966,     12,     16,     24,   1613,   7865,    283,    297,\n",
      "          21198,   6355,    653,  60244,  78669,   7377,    976,  11804,    297,\n",
      "          28352,     11,  32092,   1709,   1124,    406,   7680,     11,  22718,\n",
      "            312,  15705,  12652,  38483,   8498,    300,  33930,  15185,     11,\n",
      "          41255,    283,   2860,     13,   2160,    704,  28194,  98465,   7953,\n",
      "          27928,   2256,   1124,     77,   2724,   6496,     13,    220,   1124,\n",
      "             77,  30205,    409,   5679,    300,    409, 133893,    409,  36268,\n",
      "             71,  18244,     11,  12393,     13,   1124,     77,     36,    642,\n",
      "           8210,  19069,  12393,   5779,   5604,    438,  94215,  13596,   4441,\n",
      "           1953,  47831,   3348,   4533,    404,    308,  23318,  38323,  28352,\n",
      "             13,    468,    642,  29610,   2643,   1124,     77,    772,  18244,\n",
      "             11,   2643,  15723,  18244,     11,   2643,   6234,  37085,  23494,\n",
      "          24224,     11,  70502,   4883,  54639,    976,  10608,  40534,   1124,\n",
      "            811,  44217,    685,   4317,   5841,    438,  45962,  87003,  11107,\n",
      "          98860,    259,  16095,  28652,   7953,   1036,   4106,    268,  59477,\n",
      "            409,    451,  12058,    976,    854,   5908,   1036,     39,   9683,\n",
      "           1124,  42341,  60910,     66,  15027,    854,    976,  41398,   9804,\n",
      "          66011,     13,  14248,   4443,   1531,   1958,  20210,  17137,   2123,\n",
      "           3348,   2112,    265,    258,   1953,     12,   2301,     11,   6560,\n",
      "            674,  37215,    309,   1124,  36391,  18050,   9202,  18244,   2994,\n",
      "          72854,   4505,     11,  66441,   2543,   3878,  12541,  98424,  64756,\n",
      "           5553,   3348,    489,   7061,     13,   1124,     77,     36,  12393,\n",
      "          46667,  66441,     13,    506,  16564,     78,  85479,  45758,  77584,\n",
      "          43879,   8565,    409,  94215,  13596,  32898,   1774,   5249,    976,\n",
      "            489,    321,  93230,    409,   1124,    303,   1794,   4260,    416,\n",
      "            384,    312,    924,  83562,   9870,    369,    870,   6817,    976,\n",
      "          92824,   1709,  12393,   2229,  69847,  48176,  15111,  78707,   7377,\n",
      "           2335,     72,  43315,     11,  49624,   7841,    284,  11443,    279,\n",
      "          19966,     12,     16,     24,  27422,  48758,    279,  17857,    315,\n",
      "           7377,   4401,   2163,    279,   1879,   5267,   9217,   4035, 151645,\n",
      "            198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization\n",
    "model_inputs = tokenizer([text], return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Answers with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    458,   6203,  17847,\n",
      "             13, 151645,    198, 151644,    872,    271,   2610,    525,    458,\n",
      "           6203,  17847,     13,   1446,    990,    279,   2266,   3897,    438,\n",
      "            697,  57435,   6540,   2331,    311,   4226,    279,   3405,    624,\n",
      "           2147,    284,    508,   7524,  54436,  12854,   2893,   1210,    220,\n",
      "             15,     11,    364,   2893,   6106,   1210,    364,     16,    516,\n",
      "            364,   2427,   1210,   4927,    691,     14,   9286,   7836,  72638,\n",
      "             16,  15995,  24731,   2150,   7495,   1131,     32,    472,  79916,\n",
      "          33347,  13213,   4942,   4317,  47588,   2994,  15611,    343,  23696,\n",
      "          58194,   1124,     77,   1124,     77,     32,  12217,  21925,    653,\n",
      "          19966,     12,     16,     24,   1613,   7865,    283,    297,  21198,\n",
      "           6355,    653,  60244,  78669,   7377,    976,  11804,    297,  28352,\n",
      "             11,  32092,   1709,   1124,    406,   7680,     11,  22718,    312,\n",
      "          15705,  12652,  38483,   8498,    300,  33930,  15185,     11,  41255,\n",
      "            283,   2860,     13,   2160,    704,  28194,  98465,   7953,  27928,\n",
      "           2256,   1124,     77,   2724,   6496,     13,    220,   1124,     77,\n",
      "          30205,    409,   5679,    300,    409, 133893,    409,  36268,     71,\n",
      "          18244,     11,  12393,     13,   1124,     77,     36,    642,   8210,\n",
      "          19069,  12393,   5779,   5604,    438,  94215,  13596,   4441,   1953,\n",
      "          47831,   3348,   4533,    404,    308,  23318,  38323,  28352,     13,\n",
      "            468,    642,  29610,   2643,   1124,     77,    772,  18244,     11,\n",
      "           2643,  15723,  18244,     11,   2643,   6234,  37085,  23494,  24224,\n",
      "             11,  70502,   4883,  54639,    976,  10608,  40534,   1124,    811,\n",
      "          44217,    685,   4317,   5841,    438,  45962,  87003,  11107,  98860,\n",
      "            259,  16095,  28652,   7953,   1036,   4106,    268,  59477,    409,\n",
      "            451,  12058,    976,    854,   5908,   1036,     39,   9683,   1124,\n",
      "          42341,  60910,     66,  15027,    854,    976,  41398,   9804,  66011,\n",
      "             13,  14248,   4443,   1531,   1958,  20210,  17137,   2123,   3348,\n",
      "           2112,    265,    258,   1953,     12,   2301,     11,   6560,    674,\n",
      "          37215,    309,   1124,  36391,  18050,   9202,  18244,   2994,  72854,\n",
      "           4505,     11,  66441,   2543,   3878,  12541,  98424,  64756,   5553,\n",
      "           3348,    489,   7061,     13,   1124,     77,     36,  12393,  46667,\n",
      "          66441,     13,    506,  16564,     78,  85479,  45758,  77584,  43879,\n",
      "           8565,    409,  94215,  13596,  32898,   1774,   5249,    976,    489,\n",
      "            321,  93230,    409,   1124,    303,   1794,   4260,    416,    384,\n",
      "            312,    924,  83562,   9870,    369,    870,   6817,    976,  92824,\n",
      "           1709,  12393,   2229,  69847,  48176,  15111,  78707,   7377,   2335,\n",
      "             72,  43315,   2894,    701,  11789,  54436,  12854,   2893,   1210,\n",
      "            220,     15,     11,    364,   2893,   6106,   1210,    364,     16,\n",
      "            516,    364,   2427,   1210,   4927,    691,     14,   9286,   7836,\n",
      "          72638,     16,  15995,  24731,   2150,   7495,   1131,     32,    472,\n",
      "          79916,  33347,  13213,   4942,   4317,  47588,   2994,  15611,    343,\n",
      "          23696,  58194,   1124,     77,   1124,     77,     32,  12217,  21925,\n",
      "            653,  19966,     12,     16,     24,   1613,   7865,    283,    297,\n",
      "          21198,   6355,    653,  60244,  78669,   7377,    976,  11804,    297,\n",
      "          28352,     11,  32092,   1709,   1124,    406,   7680,     11,  22718,\n",
      "            312,  15705,  12652,  38483,   8498,    300,  33930,  15185,     11,\n",
      "          41255,    283,   2860,     13,   2160,    704,  28194,  98465,   7953,\n",
      "          27928,   2256,   1124,     77,   2724,   6496,     13,    220,   1124,\n",
      "             77,  30205,    409,   5679,    300,    409, 133893,    409,  36268,\n",
      "             71,  18244,     11,  12393,     13,   1124,     77,     36,    642,\n",
      "           8210,  19069,  12393,   5779,   5604,    438,  94215,  13596,   4441,\n",
      "           1953,  47831,   3348,   4533,    404,    308,  23318,  38323,  28352,\n",
      "             13,    468,    642,  29610,   2643,   1124,     77,    772,  18244,\n",
      "             11,   2643,  15723,  18244,     11,   2643,   6234,  37085,  23494,\n",
      "          24224,     11,  70502,   4883,  54639,    976,  10608,  40534,   1124,\n",
      "            811,  44217,    685,   4317,   5841,    438,  45962,  87003,  11107,\n",
      "          98860,    259,  16095,  28652,   7953,   1036,   4106,    268,  59477,\n",
      "            409,    451,  12058,    976,    854,   5908,   1036,     39,   9683,\n",
      "           1124,  42341,  60910,     66,  15027,    854,    976,  41398,   9804,\n",
      "          66011,     13,  14248,   4443,   1531,   1958,  20210,  17137,   2123,\n",
      "           3348,   2112,    265,    258,   1953,     12,   2301,     11,   6560,\n",
      "            674,  37215,    309,   1124,  36391,  18050,   9202,  18244,   2994,\n",
      "          72854,   4505,     11,  66441,   2543,   3878,  12541,  98424,  64756,\n",
      "           5553,   3348,    489,   7061,     13,   1124,     77,     36,  12393,\n",
      "          46667,  66441,     13,    506,  16564,     78,  85479,  45758,  77584,\n",
      "          43879,   8565,    409,  94215,  13596,  32898,   1774,   5249,    976,\n",
      "            489,    321,  93230,    409,   1124,    303,   1794,   4260,    416,\n",
      "            384,    312,    924,  83562,   9870,    369,    870,   6817,    976,\n",
      "          92824,   1709,  12393,   2229,  69847,  48176,  15111,  78707,   7377,\n",
      "           2335,     72,  43315,     11,  49624,   7841,    284,  11443,    279,\n",
      "          19966,     12,     16,     24,  27422,  48758,    279,  17857,    315,\n",
      "           7377,   4401,   2163,    279,   1879,   5267,   9217,   4035, 151645,\n",
      "            198, 151644,  77091,    198,   9454,     11,    279,  19966,     12,\n",
      "             16,     24,  27422,    702,  12824,  48758,    279,  17857,    315,\n",
      "           7377,   4401,  30450,     13,    576,  21085,   5302,    429,    330,\n",
      "             32,  12217,  21925,    653,  19966,     12,     16,     24,   1613,\n",
      "           7865,    283,    297,  21198,   6355,    653,  60244,  78669,   7377,\n",
      "            976,  11804,    297,  28352,   1189,   1096,  14807,    264,   5089,\n",
      "           5263,    304,   7377,  24376,    323,  29016,  49825,   2337,    419,\n",
      "           4168,   4152,    311,    279,  31861,    315,   8699,    975,    323,\n",
      "           1008,   2860,   7488,  39886,    504,    279,  27422,     13, 151645]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens = 512)\n",
    "\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  9454,     11,    279,  19966,     12,     16,     24,  27422,    702,\n",
      "         12824,  48758,    279,  17857,    315,   7377,   4401,  30450,     13,\n",
      "           576,  21085,   5302,    429,    330,     32,  12217,  21925,    653,\n",
      "         19966,     12,     16,     24,   1613,   7865,    283,    297,  21198,\n",
      "          6355,    653,  60244,  78669,   7377,    976,  11804,    297,  28352,\n",
      "          1189,   1096,  14807,    264,   5089,   5263,    304,   7377,  24376,\n",
      "           323,  29016,  49825,   2337,    419,   4168,   4152,    311,    279,\n",
      "         31861,    315,   8699,    975,    323,   1008,   2860,   7488,  39886,\n",
      "           504,    279,  27422,     13, 151645], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# Unpack the responses\n",
    "# Goal: Extract only the tokens generated by the model (i.e. the part of the output that comes after\n",
    "# the input tokens). This is useful because models like GPT or others based on autoregressive\n",
    "# decoding often return a concatenation of the input and output.\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the COVID-19 pandemic has indeed accelerated the pace of digital development globally. The passage states that \"A pandemia do COVID-19 acelerou o ritmo do desenvolvimento digital em todo o mundo.\" This indicates a significant increase in digital adoption and technological advancement during this period due to the necessity of remote work and other online activities arising from the pandemic.\n"
     ]
    }
   ],
   "source": [
    "# Apply the decode to get the generated text\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
